{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <ins>**TASK 2**</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Utils Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <ins>Original functions from starter_notebooks</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(file_path):\n",
    "    \"\"\"\n",
    "    Load JSON data from a file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    dict: JSON data loaded from the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        contents = json.load(file)\n",
    "    \n",
    "    return contents\n",
    "\n",
    "\n",
    "def create_tfidf_matrix(citing_dataset, nonciting_dataset, vectorizer=TfidfVectorizer()):\n",
    "    \"\"\"\n",
    "    Creates TF-IDF matrix for the given citing and non-citing datasets based on the specified text column.\n",
    "\n",
    "    Parameters:\n",
    "    citing_dataset (json)): DataFrame containing citing patents.\n",
    "    nonciting_dataset (json): DataFrame containing non-citing patents.\n",
    "    vectorizer (TfidfVectorizer, optional): TfidfVectorizer object for vectorizing text data.\n",
    "                                             Defaults to TfidfVectorizer().\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing TF-IDF matrices for citing and non-citing patents respectively.\n",
    "           (tfidf_matrix_citing, tfidf_matrix_nonciting)\n",
    "    \"\"\"\n",
    "    all_text = [patent['text'] for patent in citing_dataset + nonciting_dataset]\n",
    "\n",
    "    # Vectorizing descriptions\n",
    "    # print(\"Vectorizing descriptions...\")\n",
    "    # tfidf_matrix = vectorizer.fit_transform(tqdm(all_text, desc=\"TF-IDF\"))\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_text)\n",
    "\n",
    "    # Since we're interested in similarities between citing and cited patents,\n",
    "    # we need to split the TF-IDF matrix back into two parts\n",
    "    split_index = len(citing_dataset)\n",
    "    tfidf_matrix_citing = tfidf_matrix[:split_index]\n",
    "    tfidf_matrix_nonciting = tfidf_matrix[split_index:]\n",
    "\n",
    "    # Size of vocabulary\n",
    "    # print(\"Size of vocabulary:\", len(vectorizer.vocabulary_))\n",
    "\n",
    "    return tfidf_matrix_citing, tfidf_matrix_nonciting\n",
    "\n",
    "def create_word2vec_matrix(citing_dataset, nonciting_dataset, word2vec_model):\n",
    "    \"\"\"\n",
    "    Creates Word2Vec matrix for the given citing and non-citing datasets based on the specified Word2Vec model.\n",
    "\n",
    "    Parameters:\n",
    "    citing_dataset (list of str): List of citing patents' text.\n",
    "    nonciting_dataset (list of str): List of non-citing patents' text.\n",
    "    word2vec_model (Word2Vec): Pre-trained Word2Vec model.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing Word2Vec matrices for citing and non-citing patents respectively.\n",
    "           (word2vec_matrix_citing, word2vec_matrix_nonciting)\n",
    "    \"\"\"\n",
    "    def get_average_vector(text, model):\n",
    "        words = text.split()\n",
    "        vectors = [model.wv.get_vector(word) for word in words if word in model.wv]\n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            # If no words in the document are found in the model vocabulary,\n",
    "            # return a zero vector of the same dimensionality\n",
    "            return np.zeros(model.vector_size)\n",
    "\n",
    "    # Convert text data to Word2Vec vectors\n",
    "    word2vec_matrix_citing = np.array([get_average_vector(patent[\"text\"], word2vec_model) for patent in citing_dataset])\n",
    "    word2vec_matrix_nonciting = np.array([get_average_vector(patent[\"text\"], word2vec_model) for patent in nonciting_dataset])\n",
    "\n",
    "    return word2vec_matrix_citing, word2vec_matrix_nonciting\n",
    "\n",
    "def get_mapping_dict(mapping_df):\n",
    "    \"\"\"\n",
    "    Creates dictionary of citing ids to non-citing id based on given dataframe (which is based on providedjson)\n",
    "\n",
    "    Parameters:\n",
    "    mapping_df (DataFrame): DataFrame containing mapping between citing and cited patents\n",
    "    Returns:\n",
    "    dict: dictionary of unique citing patent ids to list of cited patent ids\n",
    "    \"\"\"\n",
    "    mapping_dict = {}\n",
    "\n",
    "    for _, row in mapping_df.iterrows():\n",
    "        key = row[0]  # Value from column 0\n",
    "        value = row[2]  # Value from column 2\n",
    "        if key in mapping_dict:\n",
    "            mapping_dict[key].append(value)\n",
    "        else:\n",
    "            mapping_dict[key] = [value]\n",
    "\n",
    "    return mapping_dict\n",
    "\n",
    "def create_corpus(corpus, text_type):\n",
    "    \"\"\"\n",
    "    Extracts text data from a corpus based on the specified text type.\n",
    "\n",
    "    Parameters:\n",
    "    corpus (list): List of dictionaries representing patent documents.\n",
    "    text_type (str): Type of text to extract ('title', 'abstract', 'claim1', 'claims', 'description', 'fulltext').\n",
    "\n",
    "    Returns:\n",
    "    list: List of dictionaries with 'id' and 'text' keys representing each document in the corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    app_ids = [doc['Application_Number'] + doc['Application_Category'] for doc in corpus]\n",
    "\n",
    "    cnt = 0 # count the number of documents without text\n",
    "    texts = []  # list of texts\n",
    "    ids_to_remove = []  # list of ids of documents without text, to remove them from the corpus\n",
    "\n",
    "    if text_type == 'title':\n",
    "        for doc in corpus:\n",
    "            try:\n",
    "                texts.append(doc['Content']['title'])\n",
    "            except: # if the document does not have a title\n",
    "                ids_to_remove.append(doc['Application_Number']+doc['Application_Category'])\n",
    "                cnt += 1\n",
    "        if cnt > 0:\n",
    "            print(f\"Number of documents without title: {cnt}\")\n",
    "\n",
    "    elif text_type == 'abstract':\n",
    "        for doc in corpus:\n",
    "            try:\n",
    "                texts.append(doc['Content']['pa01'])\n",
    "            except: # if the document does not have an abstract\n",
    "                ids_to_remove.append(doc['Application_Number']+doc['Application_Category'])\n",
    "                cnt += 1\n",
    "        if cnt > 0:\n",
    "            print(f\"Number of documents without abstract: {cnt}\")\n",
    "\n",
    "    elif text_type == 'claim1':\n",
    "        for doc in corpus:\n",
    "            try:\n",
    "                texts.append(doc['Content']['c-en-0001'])\n",
    "            except: # if the document does not have claim 1\n",
    "                ids_to_remove.append(doc['Application_Number']+doc['Application_Category'])\n",
    "                cnt += 1\n",
    "\n",
    "        if cnt > 0:\n",
    "            print(f\"Number of documents without claim 1: {cnt}\")\n",
    "\n",
    "    elif text_type == 'claims':\n",
    "        # all the values with the key starting with 'c-en-', each element in the final list is a list of claims\n",
    "        for doc in corpus:\n",
    "            doc_claims = []\n",
    "            for key in doc['Content'].keys():\n",
    "                if key.startswith('c-en-'):\n",
    "                    doc_claims.append(doc['Content'][key])\n",
    "            if len(doc_claims) == 0:    # if the document does not have any claims\n",
    "                ids_to_remove.append(doc['Application_Number']+doc['Application_Category'])\n",
    "                cnt += 1\n",
    "            else:\n",
    "                doc_text_string = ' '.join(doc_claims)\n",
    "                texts.append(doc_text_string)\n",
    "        if cnt > 0:\n",
    "            print(f\"Number of documents without claims: {cnt}\")\n",
    "\n",
    "    elif text_type == 'description':\n",
    "        # all the values with the key starting with 'p'\n",
    "        for doc in corpus:\n",
    "            doc_text = []\n",
    "            for key in doc['Content'].keys():\n",
    "                if key.startswith('p'):\n",
    "                    doc_text.append(doc['Content'][key])\n",
    "            if len(doc_text) == 0:  # if the document does not have any description\n",
    "                ids_to_remove.append(doc['Application_Number']+doc['Application_Category'])\n",
    "                cnt += 1\n",
    "            else:\n",
    "                doc_text_string = ' '.join(doc_text)\n",
    "                texts.append(doc_text_string)\n",
    "        if cnt > 0:\n",
    "            print(f\"Number of documents without description: {cnt}\")\n",
    "\n",
    "    elif text_type == 'fulltext':\n",
    "        for doc in corpus:\n",
    "            doc_text = list(doc['Content'].values())\n",
    "            doc_text_string = ' '.join(doc_text)\n",
    "            texts.append(doc_text_string)\n",
    "        if cnt > 0:\n",
    "            print(f\"Number of documents without any text: {cnt}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid text type\")\n",
    "\n",
    "    if len(ids_to_remove) > 0:\n",
    "        print(f\"Removing {len(ids_to_remove)} documents without required text\")\n",
    "        for id_ in ids_to_remove[::-1]:\n",
    "            idx = app_ids.index(id_)\n",
    "            del app_ids[idx]\n",
    "\n",
    "    # Create a list of dictionaries with app_ids and texts\n",
    "    corpus_data = [{'id': app_id, 'text': text} for app_id, text in zip(app_ids, texts)]\n",
    "\n",
    "    return corpus_data\n",
    "\n",
    "\n",
    "def get_true_and_predicted(citing_to_cited_dict, recommendations_dict):\n",
    "    \"\"\"\n",
    "    Get the true and predicted labels for the metrics calculation.\n",
    "\n",
    "    Parameters:\n",
    "    citing_to_cited_dict : dict of str : list of str\n",
    "        Mapping between citing patents and the list of their cited patents\n",
    "    recommendations_dict : dict of str : list of str\n",
    "        Mapping between citing patents and the sorted list of recommended patents\n",
    "\n",
    "    Returns:\n",
    "    list of list\n",
    "        True relevant items for each recommendation list.\n",
    "    list of list\n",
    "        Predicted recommended items for each recommendation list.\n",
    "    int\n",
    "        Number of patents not in the citation mapping\n",
    "    \"\"\"\n",
    "    # Initialize lists to store true labels and predicted labels\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    not_in_citation_mapping = 0\n",
    "\n",
    "    # Iterate over the items in both dictionaries\n",
    "    for citing_id in recommendations_dict.keys():\n",
    "        # Check if the citing_id is present in both dictionaries\n",
    "        if citing_id in citing_to_cited_dict:\n",
    "            # If yes, append the recommended items from both dictionaries to the respective lists\n",
    "            true_labels.append(citing_to_cited_dict[citing_id])\n",
    "            predicted_labels.append(recommendations_dict[citing_id])\n",
    "        else:\n",
    "            not_in_citation_mapping += 1\n",
    "\n",
    "    return true_labels, predicted_labels, not_in_citation_mapping\n",
    "\n",
    "\n",
    "\n",
    "def mean_recall_at_k(true_labels, predicted_labels, k=10):\n",
    "    \"\"\"\n",
    "    Calculate the mean Recall@k for a list of recommendations.\n",
    "\n",
    "    Parameters:\n",
    "    true_labels : list of list\n",
    "        True relevant items for each recommendation list.\n",
    "    predicted_labels : list of list\n",
    "        Predicted recommended items for each recommendation list.\n",
    "    k : int\n",
    "        Number of recommendations to consider.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        Mean Recall@k value.\n",
    "    \"\"\"\n",
    "    recalls_at_k = []\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        # Calculate Recall@k for each recommendation list\n",
    "        true_set = set(true)\n",
    "        k = min(k, len(pred))\n",
    "        relevant_count = sum(1 for item in pred[:k] if item in true_set)\n",
    "        recalls_at_k.append(relevant_count / len(true_set))\n",
    "\n",
    "    # Calculate the mean Recall@k\n",
    "    mean_recall = sum(recalls_at_k) / len(recalls_at_k)\n",
    "\n",
    "    return mean_recall\n",
    "\n",
    "def mean_inv_ranking(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculate the mean of lists of the mean inverse rank of true relevant items\n",
    "    in the lists of sorted recommended items.\n",
    "\n",
    "    Parameters:\n",
    "    true_labels : list of list\n",
    "        True relevant items for each recommendation list.\n",
    "    predicted_labels : list of list\n",
    "        Predicted recommended items for each recommendation list.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        Mean of lists of the mean inverse rank of true relevant items.\n",
    "    \"\"\"\n",
    "    mean_ranks = []\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        # Calculate the inverse rank of true relevant items\n",
    "        # in the recommendation list\n",
    "        ranks = []\n",
    "        for item in true:\n",
    "            try:\n",
    "                rank = 1 / (pred.index(item) + 1)\n",
    "            except ValueError:\n",
    "                rank = 0  # If item not found, assign 0\n",
    "            ranks.append(rank)\n",
    "\n",
    "        # Calculate the mean inverse rank of true relevant items\n",
    "        # in the recommendation list\n",
    "        mean_rank = sum(ranks) / len(ranks)\n",
    "        mean_ranks.append(mean_rank)\n",
    "\n",
    "    # Calculate the mean of the mean inverse ranks across all recommendation lists\n",
    "    mean_of_mean_ranks = sum(mean_ranks) / len(mean_ranks)\n",
    "\n",
    "    return mean_of_mean_ranks\n",
    "\n",
    "\n",
    "def mean_ranking(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculate the mean of lists of the mean rank of true relevant items\n",
    "    in the lists of sorted recommended items.\n",
    "\n",
    "    Parameters:\n",
    "    true_labels : list of list\n",
    "        True relevant items for each recommendation list.\n",
    "    predicted_labels : list of list\n",
    "        Predicted recommended items for each recommendation list.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        Mean of lists of the mean rank of true relevant items.\n",
    "    \"\"\"\n",
    "    mean_ranks = []\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        # Calculate the rank of true relevant items\n",
    "        # in the recommendation list\n",
    "        ranks = []\n",
    "        for item in true:\n",
    "            try:\n",
    "                rank = pred.index(item) + 1\n",
    "            except ValueError:\n",
    "                rank = len(pred)  # If item not found, assign the length of the list\n",
    "            ranks.append(rank)\n",
    "\n",
    "        # Calculate the mean rank of true relevant items\n",
    "        # in the recommendation list\n",
    "        mean_rank = sum(ranks) / len(ranks)\n",
    "        mean_ranks.append(mean_rank)\n",
    "\n",
    "    # Calculate the mean of the mean ranks across all recommendation lists\n",
    "    mean_of_mean_ranks = sum(mean_ranks) / len(mean_ranks)\n",
    "\n",
    "    return mean_of_mean_ranks\n",
    "\n",
    "\n",
    "\n",
    "def mean_average_precision(true_labels, predicted_labels, k=10):\n",
    "    \"\"\"\n",
    "    Calculate the mean Average Precision for a list of recommendations.\n",
    "\n",
    "    Parameters:\n",
    "    true_labels : list of list\n",
    "        True relevant items for each recommendation list.\n",
    "    predicted_labels : list of list\n",
    "        Predicted recommended items for each recommendation list.\n",
    "    k : int\n",
    "        Number of recommendations to consider.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        Mean Average Precision value.\n",
    "    \"\"\"\n",
    "    average_precisions = []\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        # Calculate Average Precision for each recommendation list\n",
    "        true_set = set(true)\n",
    "        precision_at_k = []\n",
    "        relevant_count = 0\n",
    "        for i, item in enumerate(pred[:k]):\n",
    "            if item in true_set:\n",
    "                relevant_count += 1\n",
    "                precision_at_k.append(relevant_count / (i + 1))\n",
    "        average_precision = sum(precision_at_k) / len(true_set)\n",
    "        average_precisions.append(average_precision)\n",
    "\n",
    "    # Calculate the mean Average Precision\n",
    "    mean_average_precision = sum(average_precisions) / len(average_precisions)\n",
    "\n",
    "    return mean_average_precision\n",
    "\n",
    "def top_k_ranks(citing, cited, cosine_similarities, k=10):\n",
    "    # Create a dictionary to store the top k ranks for each citing patent\n",
    "    top_k_ranks = {}\n",
    "    for i, content_id in enumerate(citing):\n",
    "        top_k_ranks[content_id['id']] = [cited[j]['id'] for j in np.argsort(cosine_similarities[i])[::-1][:k]]\n",
    "    return top_k_ranks\n",
    "\n",
    "def get_true_and_predicted_task_2(citation_to_paragraph_dict, recommendations_dict):\n",
    "\n",
    "    \"\"\"\n",
    "    Get the true and predicted labels for the metrics calculation.\n",
    "\n",
    "    Parameters:\n",
    "    citation_to_paragraph_dict :\n",
    "        dict of str : list of str\n",
    "        Mapping between the concatenation of citing and cited id and the list of their paragraphs\n",
    "    recommendations_dict :\n",
    "        dict of str : list of str\n",
    "        Mapping between the concatenation of citing and cited id and the sorted list of recommended paragraphs\n",
    "\n",
    "    Returns:\n",
    "    list of list\n",
    "        True relevant items for each recommendation list.\n",
    "    list of list\n",
    "        Predicted recommended items sorted for each recommendation list.\n",
    "    int\n",
    "        Number of pairs of citing and cited patents not in the citation mapping\n",
    "    \"\"\"\n",
    "    # Initialize lists to store true labels and predicted labels\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    not_in_citation_mapping = 0\n",
    "\n",
    "    # Iterate over the items in both dictionaries\n",
    "    for id in citation_to_paragraph_dict.keys():\n",
    "        # Check if the pair of citing_id and cited_id is present in both dictionaries\n",
    "        true_labels.append(citation_to_paragraph_dict[id])\n",
    "        if id in recommendations_dict:\n",
    "            predicted_labels.append(recommendations_dict[id])\n",
    "        else:\n",
    "            predicted_labels.append([])\n",
    "            not_in_citation_mapping += 1\n",
    "\n",
    "    return true_labels, predicted_labels, not_in_citation_mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <ins>Customs functions</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords=True, stem=False):\n",
    "    \"\"\"\n",
    "    Clean the text data by converting to lowercase, removing stopwords, stemming, and removing special characters.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Text data to clean.\n",
    "    remove_stopwords (bool, optional): Whether to remove stopwords. Defaults to True.\n",
    "    stem (bool, optional): Whether to perform stemming. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    str: Cleaned text data.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    text = re.sub(r'\\(([0-9])+\\)', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    \n",
    "    if stem:\n",
    "        ps = PorterStemmer()\n",
    "        text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean_corpus(corpus, verbose=True, stem=False):\n",
    "    \"\"\"\n",
    "    Clean the text data in the corpus by converting to lowercase, removing stopwords, stemming, and removing special characters.\n",
    "\n",
    "    Parameters:\n",
    "    corpus (list): List of dictionaries representing patent documents.\n",
    "    verbose (bool, optional): Whether to show progress bar. Defaults to True.\n",
    "    stem (bool, optional): Whether to perform stemming. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    list: List of dictionaries with 'id' and 'text' keys representing each document in the corpus.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            corpus_clean = list(tqdm(executor.map(clean_text, [patent['text'] for patent in corpus]), total=len(corpus), desc=\"Cleaning\"))\n",
    "        \n",
    "        return [{'id': patent['id'], 'text': text} for patent, text in zip(corpus, corpus_clean)]\n",
    "\n",
    "    else:\n",
    "        return [{'id': patent['id'], 'text': clean_text(patent['text'], stem=stem)} for patent in corpus]\n",
    "\n",
    "def split_text_w2v(cited_corpus, citing_corpus):\n",
    "    \"\"\"\n",
    "    Split text data into sentences for Word2Vec training.\n",
    "\n",
    "    Parameters:\n",
    "    cited_corpus (dict): Dictionary containing cited patent paragraphs.\n",
    "    citing_paragraphs (dict): Dictionary containing citing patent paragraphs.\n",
    "\n",
    "    Returns:\n",
    "    list: List of lists of sentences for Word2Vec training.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "\n",
    "    for k in cited_corpus.keys():\n",
    "        for paragraph in cited_corpus[k]:\n",
    "            sentences.append(nltk.sent_tokenize(paragraph[\"text\"]))\n",
    "\n",
    "        sentences.append(nltk.sent_tokenize(citing_corpus[k][0][\"text\"]))\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def get_y_true(mapping_dataset_df):\n",
    "    \"\"\"\n",
    "    Get the true labels for the citation mapping dataset.\n",
    "\n",
    "    Parameters:\n",
    "    mapping_dataset_df (DataFrame): DataFrame containing the citation mapping dataset.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary of citing and cited patent pairs to the citation label.\n",
    "    \"\"\"\n",
    "    y_true = {}\n",
    "    for i, rows in mapping_dataset_df.iterrows():\n",
    "        key = rows[0] + \"_\" + rows[2]\n",
    "        y_true[key] = rows[3]\n",
    "\n",
    "    return y_true\n",
    "\n",
    "def create_pairs(data, n_cited=10):\n",
    "    \"\"\"\n",
    "    Create pairs of citing and cited patents from the given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    data (dict): Dictionary containing the dataset.\n",
    "    n_cidted (int, optional): Number of cited patents to consider. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "    list: List of tuples representing citing and cited patent pairs.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "\n",
    "    for key in data.keys():\n",
    "        for i, value in enumerate(data[key]):\n",
    "            pairs.append((key, value))\n",
    "\n",
    "            if i == n_cited - 1:\n",
    "                break\n",
    "\n",
    "    return pairs\n",
    "\n",
    "def get_cited_corpus(val):\n",
    "    \"\"\"\n",
    "    Get the cited paragraphs from the given patent document.\n",
    "\n",
    "    Parameters:\n",
    "    val (dict): Dictionary containing the patent document.\n",
    "\n",
    "    Returns:\n",
    "    list: List of cited paragraphs.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    paragraphs = []\n",
    "\n",
    "    for k in val.keys():\n",
    "        if k.startswith('p') or k.startswith('c'):\n",
    "            paragraphs.append(val[k])\n",
    "            ids.append(k)\n",
    "\n",
    "    return ids, paragraphs\n",
    "\n",
    "def get_citing_corpus(val):\n",
    "    \"\"\"\n",
    "    Get the citing paragraphs from the given patent document.\n",
    "\n",
    "    Parameters:\n",
    "    val (dict): Dictionary containing the patent document.\n",
    "\n",
    "    Returns:\n",
    "    str: Concatenated citing paragraphs.\n",
    "    \"\"\"\n",
    "    paragraphs = []\n",
    "\n",
    "    for k in val.keys():\n",
    "        paragraphs.append(val[k])\n",
    "\n",
    "    return \" \".join(paragraphs)\n",
    "\n",
    "def get_citing_paragraphs(pairs, df):\n",
    "    \"\"\"\n",
    "    Get the citing paragraphs from the given citing patent dataset.\n",
    "\n",
    "    Parameters:\n",
    "    pairs (list): List of citing and cited patent pairs.\n",
    "\n",
    "    Returns:\n",
    "    list: List of citing patent ids.\n",
    "    \"\"\"\n",
    "    citing_ids = np.unique([e[0] for e in pairs]).tolist()\n",
    "    all_citing_paragraphs = []\n",
    "\n",
    "    for citing in tqdm(citing_ids, desc=\"get citing paragraphs\"):\n",
    "        citing_text = df[df['Application_Number'] == citing[:-2]][\"Content\"].values[0]\n",
    "        descriptions = get_citing_corpus(citing_text)\n",
    "        all_citing_paragraphs += [descriptions]\n",
    "\n",
    "    return citing_ids, all_citing_paragraphs\n",
    "\n",
    "def get_cited_paragraphs(pairs, nonciting_dataset_df):\n",
    "    \"\"\"\n",
    "    Get the cited paragraphs from the given cited patent dataset.\n",
    "\n",
    "    Parameters:\n",
    "    pairs (list): List of citing and cited patent pairs.\n",
    "    nonciting_dataset_df (DataFrame): DataFrame containing the non-citing patent dataset.\n",
    "\n",
    "    Returns:\n",
    "    list: List of cited patent ids.\n",
    "    \"\"\"\n",
    "    all_cited_ids = []\n",
    "    all_cited_paragraphs = []\n",
    "    all_type_of_text = []\n",
    "\n",
    "    for citing, cited in tqdm(pairs, desc=\"get cited paragraphs\"):\n",
    "        cited_text = nonciting_dataset_df[nonciting_dataset_df['Application_Number'] == cited[:-2]]['Content'].values[0]\n",
    "        cited_ids, cited_paragraphs = get_cited_corpus(cited_text)\n",
    "        all_cited_paragraphs += [cited_paragraphs]\n",
    "        all_cited_ids += [cited]\n",
    "        all_type_of_text += [cited_ids]\n",
    "\n",
    "\n",
    "    return all_cited_ids, all_cited_paragraphs, all_type_of_text\n",
    "\n",
    "def map_ids_to_text(ids, texts):\n",
    "    \"\"\"\n",
    "    Map patent ids to their corresponding text data.\n",
    "\n",
    "    Parameters:\n",
    "    ids (list): List of patent ids.\n",
    "    texts (list): List of text data.\n",
    "\n",
    "    Returns:\n",
    "    list: List of dictionaries containing patent ids and text data.\n",
    "    \"\"\"\n",
    "    mapping = []\n",
    "\n",
    "    for i, id in enumerate(ids):\n",
    "        mapping.append({\"id\": id, \"text\": texts[i]})\n",
    "\n",
    "    return mapping\n",
    "\n",
    "def ind_citing(citing_dataset, citing_id):\n",
    "    \"\"\"\n",
    "    Get the index of the citing patent in the citing dataset.\n",
    "\n",
    "    Parameters:\n",
    "    citing_dataset (list): List of citing patents.\n",
    "    citing_id (str): Citing patent id.\n",
    "\n",
    "    Returns:\n",
    "    int: Index of the citing patent in the citing dataset.\n",
    "    \"\"\"\n",
    "    for i, citing in enumerate(citing_dataset):\n",
    "        if citing['id'] == citing_id:\n",
    "            return i\n",
    "\n",
    "    return -1\n",
    "\n",
    "def predict_pipeline(train_pairs, corpus_citing, corpus_cited, create_matrix, vectorizer, k=100):\n",
    "    \"\"\"\n",
    "    Predict citing patents\n",
    "\n",
    "    Parameters:\n",
    "    train_pairs (list): List of citing and cited patent pairs.\n",
    "    corpus_citing (list): List of citing patent paragraphs.\n",
    "    corpus_cited (list): List of cited patent paragraphs.\n",
    "    create_matrix (function): Function to create the matrix for the model.\n",
    "    vectorizer (TfidfVectorizer): TF-IDF vectorizer.\n",
    "    k (int, optional): Number of recommendations to consider. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary of citing patent ids to the sorted list of recommended cited patent ids.\n",
    "    \"\"\"\n",
    "    pred = {}\n",
    "\n",
    "    for citing, cited in tqdm(train_pairs, total=len(train_pairs)):\n",
    "        key = citing + \"_\" + cited\n",
    "        corpus_cited_sub = corpus_cited[key]\n",
    "        corpus_citing_sub = corpus_citing[key]\n",
    "\n",
    "        matrix_citing, matrix_cited = create_matrix(corpus_citing_sub, corpus_cited_sub, vectorizer)\n",
    "        cosine_similarities = linear_kernel(matrix_citing, matrix_cited)\n",
    "        top_k_ranks_dict = top_k_ranks(corpus_citing_sub, corpus_cited_sub, cosine_similarities, k=k)\n",
    "\n",
    "        pred[key] = top_k_ranks_dict[citing]\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **I. Prepare data**\n",
    "\n",
    "<ins>Explaination of the data preparation steps:</ins>\n",
    "1. For each citing patent, we concatenate all text data into one string. This includes the title, abstract, claims and paragraphs.\n",
    "2. For each cited patent corresponding to the citing patent, we store paragraphs and claims to later perform a ranking.\n",
    "3. We clean the text data by removing `special characters`, `digits`, and `stopwords`.\n",
    "4. We perform `stemming` on the text data, to reduce words to their root form.\n",
    "\n",
    "After the data preparation steps, we obtain two dictionaries:\n",
    "1. `citing_dict`: A dictionary where the key is the patent number and the value is the concatenated text data.\n",
    "2. `cited_dict`: A dictionary where the key is the patent number and the value contains paragraphs and claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_citing_train = load_json_data(\"./datasets/Content_JSONs/Citing_2020_Cleaned_Content_12k/Citing_Train_Test/citing_TRAIN.json\")\n",
    "json_citing_test = load_json_data(\"./datasets/Content_JSONs/Citing_2020_Cleaned_Content_12k/Citing_Train_Test/citing_TEST.json\")\n",
    "\n",
    "json_nonciting = load_json_data(\"./datasets/Content_JSONs/Cited_2020_Uncited_2010-2019_Cleaned_Content_22k/CLEANED_CONTENT_DATASET_cited_patents_by_2020_uncited_2010-2019.json\")\n",
    "json_citing_to_cited = load_json_data(\"./datasets/Citation_JSONs/Citation_Train.json\")\n",
    "json_citing_id = load_json_data(\"./Citing_ID_List_Test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "citing_dataset_df = pd.DataFrame(json_citing_train)\n",
    "nonciting_dataset_df = pd.DataFrame(json_nonciting)\n",
    "mapping_dataset_df = pd.DataFrame(json_citing_to_cited)\n",
    "\n",
    "train_mapping_dict = get_mapping_dict(mapping_dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88153a4aa05d4315a79d95ad6117eaeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get citing paragraphs:   0%|          | 0/6831 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb4c13f8ee84052b2b1a609a0282c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get cited paragraphs:   0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a75c69a5934051b72853d3c01e6531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning:   0%|          | 0/6831 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e901b68dbfd6490d8911f778f9c33032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Corpus Dictionaries:   0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get all train pairs (id_citing, id_cited)\n",
    "train_pairs = create_pairs(train_mapping_dict)\n",
    "\n",
    "# Get paragraphs of citing and cited\n",
    "citing_ids, citing_paragraphs = get_citing_paragraphs(train_pairs, citing_dataset_df)\n",
    "cited_ids, cited_corpus, cited_type = get_cited_paragraphs(train_pairs, nonciting_dataset_df)\n",
    "\n",
    "# Create the citing corpus\n",
    "citing_corpus = map_ids_to_text(citing_ids, citing_paragraphs)\n",
    "\n",
    "# Clean the citing corpus\n",
    "citing_corpus = clean_corpus(citing_corpus, stem=True)\n",
    "\n",
    "# Create the citing and cited corpus dictionaries\n",
    "# Key: id_citing + \"_\" + id_cited\n",
    "corpus_citing_sub_dict = {}\n",
    "corpus_cited_sub_dict = {}\n",
    "\n",
    "for i, (citing, cited) in tqdm(enumerate(train_pairs), total=len(train_pairs), desc=\"Creating Corpus Dictionaries\"):\n",
    "    key = citing + \"_\" + cited\n",
    "\n",
    "    corpus_cited_sub = map_ids_to_text(cited_type[i], cited_corpus[i])\n",
    "    corpus_citing_sub = [citing_corpus[ind_citing(citing_corpus, citing)]]\n",
    "\n",
    "    # Clean the cited corpus\n",
    "    corpus_cited_sub = clean_corpus(corpus_cited_sub, verbose=False, stem=True)\n",
    "\n",
    "    # Add citing and cited corpus to the dictionaries\n",
    "    corpus_citing_sub_dict[key] = corpus_citing_sub\n",
    "    corpus_cited_sub_dict[key] = corpus_cited_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the citing and cited corpus dictionaries to save time\n",
    "# with open(\"datasets/corpus_citing_sub_dict.json\", \"w\") as file:\n",
    "#     json.dump(corpus_citing_sub_dict, file)\n",
    "\n",
    "# with open(\"datasets/corpus_cited_sub_dict.json\", \"w\") as file:\n",
    "#     json.dump(corpus_cited_sub_dict, file)\n",
    "\n",
    "# Load the citing and cited corpus dictionaries\n",
    "with open(\"datasets/corpus_citing_sub_dict.json\", \"r\") as file:\n",
    "    corpus_citing_sub_dict = json.load(file)\n",
    "\n",
    "with open(\"datasets/corpus_cited_sub_dict.json\", \"r\") as file:\n",
    "    corpus_cited_sub_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the true labels for the citation mapping dataset\n",
    "y_true = get_y_true(mapping_dataset_df)\n",
    "\n",
    "# Create pairs of citing and cited patents\n",
    "train_pairs = create_pairs(train_mapping_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **II. Basic Count Vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389ef21161084175adb11eae941d4a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average precision (k=10):  0.11951776095505955\n",
      "Mean average precision (k=100):  0.31461492981372485\n"
     ]
    }
   ],
   "source": [
    "# 1m30\n",
    "# Predicting using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "y_pred = predict_pipeline(train_pairs, corpus_citing_sub_dict, corpus_cited_sub_dict, create_tfidf_matrix, vectorizer)\n",
    "\n",
    "# Calculate metrics\n",
    "y_true = get_y_true(mapping_dataset_df)\n",
    "true_labels, predicted_labels, not_in_citation_mapping = get_true_and_predicted_task_2(y_true, y_pred)\n",
    "print(\"Mean average precision (k=10): \", mean_average_precision(true_labels, predicted_labels, k=10))\n",
    "print(\"Mean average precision (k=100): \", mean_average_precision(true_labels, predicted_labels, k=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c432e7b220d44dfe8a7f0edbd5f04725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average precision (k=10):  0.14377675730094247\n",
      "Mean average precision (k=100):  0.34109617938791215\n"
     ]
    }
   ],
   "source": [
    "# 1m30\n",
    "# Predicting using CountVectorizer\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "y_pred = predict_pipeline(train_pairs, corpus_citing_sub_dict, corpus_cited_sub_dict, create_tfidf_matrix, vectorizer)\n",
    "\n",
    "# Calculate metrics\n",
    "y_true = get_y_true(mapping_dataset_df)\n",
    "true_labels, predicted_labels, not_in_citation_mapping = get_true_and_predicted_task_2(y_true, y_pred)\n",
    "print(\"Mean average precision (k=10): \", mean_average_precision(true_labels, predicted_labels, k=10))\n",
    "print(\"Mean average precision (k=100): \", mean_average_precision(true_labels, predicted_labels, k=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **III. Basic TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ad8017a8e440618ed129fc703551bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average precision (k=10):  0.08909157880725094\n",
      "Mean average precision (k=100):  0.2749577513529771\n"
     ]
    }
   ],
   "source": [
    "# Predicting using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "y_pred = predict_pipeline(train_pairs, corpus_citing_sub_dict, corpus_cited_sub_dict, create_tfidf_matrix, vectorizer)\n",
    "\n",
    "# Calculate metrics\n",
    "y_true = get_y_true(mapping_dataset_df)\n",
    "true_labels, predicted_labels, not_in_citation_mapping = get_true_and_predicted_task_2(y_true, y_pred)\n",
    "print(\"Mean average precision (k=10): \", mean_average_precision(true_labels, predicted_labels, k=10))\n",
    "print(\"Mean average precision (k=100): \", mean_average_precision(true_labels, predicted_labels, k=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IV. Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Word2Vec model\n",
    "sentences = split_text_w2v(cited_corpus, citing_paragraphs)\n",
    "w2v = Word2Vec(sentences=sentences, vector_size=50, min_count=10, workers=4)\n",
    "w2v.save(\"word2vec.model\")\n",
    "\n",
    "# Predict using Word2Vec\n",
    "# w2v = Word2Vec.load(\"word2vec.model\")\n",
    "y_pred = predict_pipeline(train_pairs, corpus_citing_sub_dict, corpus_cited_sub_dict, create_word2vec_matrix, w2v)\n",
    "\n",
    "# Calculate metrics\n",
    "y_true = get_y_true(mapping_dataset_df)\n",
    "true_labels, predicted_labels, not_in_citation_mapping = get_true_and_predicted_task_2(y_true, y_pred)\n",
    "print(\"Mean average precision (k=10): \", mean_average_precision(true_labels, predicted_labels, k=10))\n",
    "print(\"Mean average precision (k=100): \", mean_average_precision(true_labels, predicted_labels, k=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **V. Fine-tune TF-IDF with Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 15:14:10,707] A new study created in memory with name: no-name-78ca1beb-1108-44f9-b684-73d3d8171df8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc4f3dbea044ba48260631f575c0bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TF-IDF Predicting:   0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 15:19:57,690] Trial 0 finished with value: 0.06386463802374108 and parameters: {'sublinear_tf': False, 'norm': 'l1', 'max_features': 6626, 'stop_words': 'english', 'n_gram_lower': 2, 'n_gram_upper': 3}. Best is trial 0 with value: 0.06386463802374108.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663125ce68f34ff5a716c89f7c769796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TF-IDF Predicting:   0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 15:23:36,289] Trial 1 finished with value: 0.05476813803532983 and parameters: {'sublinear_tf': False, 'norm': 'l1', 'max_features': 8773, 'stop_words': None, 'n_gram_lower': 3, 'n_gram_upper': 2}. Best is trial 0 with value: 0.06386463802374108.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6534dceb329f4c589b8152f1a21f84bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TF-IDF Predicting:   0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 15:30:04,921] Trial 2 finished with value: 0.06259411026390539 and parameters: {'sublinear_tf': False, 'norm': 'l1', 'max_features': 7412, 'stop_words': 'english', 'n_gram_lower': 1, 'n_gram_upper': 3}. Best is trial 0 with value: 0.06386463802374108.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51fe1e198ec4b3b907ef3ae078c10e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TF-IDF Predicting:   0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 15:33:38,399] Trial 3 finished with value: 0.0598311855738742 and parameters: {'sublinear_tf': True, 'norm': 'l2', 'max_features': 5898, 'stop_words': None, 'n_gram_lower': 3, 'n_gram_upper': 2}. Best is trial 0 with value: 0.06386463802374108.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099c3ea245fc49a69b4e132677696e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TF-IDF Predicting:   0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 15:37:07,875] Trial 4 finished with value: 0.04932945627562682 and parameters: {'sublinear_tf': True, 'norm': 'l1', 'max_features': 6839, 'stop_words': None, 'n_gram_lower': 3, 'n_gram_upper': 2}. Best is trial 0 with value: 0.06386463802374108.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf7f1e3966e416a8b146a08c78f781a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TF-IDF Predicting:   0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 15:40:57,981] Trial 5 finished with value: 0.05812304382293742 and parameters: {'sublinear_tf': False, 'norm': 'l1', 'max_features': 9142, 'stop_words': None, 'n_gram_lower': 1, 'n_gram_upper': 2}. Best is trial 0 with value: 0.06386463802374108.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796b126d62a542e394a618a449bed3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TF-IDF Predicting:   0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 15:43:58,222] Trial 6 finished with value: 0.047296627783998 and parameters: {'sublinear_tf': True, 'norm': 'l1', 'max_features': 5966, 'stop_words': None, 'n_gram_lower': 2, 'n_gram_upper': 2}. Best is trial 0 with value: 0.06386463802374108.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68bbfe648224e8cbcd4930fd118b6ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TF-IDF Predicting:   0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 15:47:32,158] Trial 7 finished with value: 0.05966976983726978 and parameters: {'sublinear_tf': True, 'norm': 'l2', 'max_features': 7052, 'stop_words': None, 'n_gram_lower': 3, 'n_gram_upper': 2}. Best is trial 0 with value: 0.06386463802374108.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcae71567eed46fd95d86f63eaeb712d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TF-IDF Predicting:   0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 15:53:49,496] Trial 8 finished with value: 0.11279120534194183 and parameters: {'sublinear_tf': True, 'norm': 'l2', 'max_features': 4603, 'stop_words': 'english', 'n_gram_lower': 1, 'n_gram_upper': 3}. Best is trial 8 with value: 0.11279120534194183.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13070fe6d9a428bba87e14c07567ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TF-IDF Predicting:   0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 15:56:40,274] Trial 9 finished with value: 0.08202457071281724 and parameters: {'sublinear_tf': True, 'norm': 'l2', 'max_features': 7806, 'stop_words': 'english', 'n_gram_lower': 2, 'n_gram_upper': 2}. Best is trial 8 with value: 0.11279120534194183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'sublinear_tf': True, 'norm': 'l2', 'max_features': 4603, 'stop_words': 'english', 'n_gram_lower': 1, 'n_gram_upper': 3}\n",
      "Best MAP@10: 0.11279120534194183\n"
     ]
    }
   ],
   "source": [
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to optimize\n",
    "    sublinear_tf = trial.suggest_categorical(\"sublinear_tf\", [True, False])\n",
    "    norm = trial.suggest_categorical(\"norm\", ['l1', 'l2'])\n",
    "    max_features = trial.suggest_int(\"max_features\", 1000, 10000)\n",
    "    stop_words = trial.suggest_categorical(\"stop_words\", ['english', None])\n",
    "    n_gram_lower = trial.suggest_int(\"n_gram_lower\", 1, 3)\n",
    "    n_gram_upper = trial.suggest_int(\"n_gram_upper\", 1, 3)\n",
    "\n",
    "    if n_gram_upper < n_gram_lower:\n",
    "        n_gram_upper = n_gram_lower\n",
    "\n",
    "    # Create the vectorizer with the hyperparameters\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=sublinear_tf, \n",
    "                                 norm=norm, \n",
    "                                 max_features=max_features, \n",
    "                                 stop_words=stop_words, \n",
    "                                 ngram_range=(n_gram_lower, n_gram_upper))\n",
    "\n",
    "    # Predict using the pipeline\n",
    "    y_pred = predict_pipeline(train_pairs, corpus_citing_sub_dict, corpus_cited_sub_dict, vectorizer)\n",
    "    true_labels, predicted_labels, _ = get_true_and_predicted_task_2(y_true, y_pred)\n",
    "    map_10 = mean_average_precision(true_labels, predicted_labels, k=10)\n",
    "    \n",
    "    return map_10\n",
    "\n",
    "# Create the study and optimize the objective function\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Print the best hyperparameters and the best MAP@10\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best MAP@10:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VI. Predictions on test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ad8017a8e440618ed129fc703551bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average precision (k=10):  0.08909157880725094\n",
      "Mean average precision (k=100):  0.2749577513529771\n"
     ]
    }
   ],
   "source": [
    "# Predicting using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "y_pred = predict_pipeline(train_pairs, corpus_citing_sub_dict, corpus_cited_sub_dict, create_tfidf_matrix, vectorizer)\n",
    "\n",
    "# Calculate metrics\n",
    "y_true = get_y_true(mapping_dataset_df)\n",
    "true_labels, predicted_labels, not_in_citation_mapping = get_true_and_predicted_task_2(y_true, y_pred)\n",
    "print(\"Mean average precision (k=10): \", mean_average_precision(true_labels, predicted_labels, k=10))\n",
    "print(\"Mean average precision (k=100): \", mean_average_precision(true_labels, predicted_labels, k=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9ecfbf8f3f4560b01248e008af8b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average precision (k=10):  0.12540232509610746\n",
      "Mean average precision (k=100):  0.31538537701093666\n"
     ]
    }
   ],
   "source": [
    "# Predicting using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(binary=True)\n",
    "y_pred = predict_pipeline(train_pairs, corpus_citing_sub_dict, corpus_cited_sub_dict, create_tfidf_matrix, vectorizer)\n",
    "\n",
    "# Calculate metrics\n",
    "y_true = get_y_true(mapping_dataset_df)\n",
    "true_labels, predicted_labels, not_in_citation_mapping = get_true_and_predicted_task_2(y_true, y_pred)\n",
    "print(\"Mean average precision (k=10): \", mean_average_precision(true_labels, predicted_labels, k=10))\n",
    "print(\"Mean average precision (k=100): \", mean_average_precision(true_labels, predicted_labels, k=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IV. Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167801f0a6634061b2c50cb0399bda71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average precision (k=10):  0.03848086624093492\n",
      "Mean average precision (k=100):  0.1938707256086945\n"
     ]
    }
   ],
   "source": [
    "# Fit the Word2Vec model\n",
    "# sentences = split_text_w2v(corpus_cited_sub_dict, corpus_citing_sub_dict)\n",
    "# w2v = Word2Vec(sentences=sentences, vector_size=50, min_count=10, workers=4)\n",
    "# w2v.save(\"word2vec.model\")\n",
    "\n",
    "# Predict using Word2Vec\n",
    "w2v = Word2Vec.load(\"word2vec.model\")\n",
    "y_pred = predict_pipeline(train_pairs, corpus_citing_sub_dict, corpus_cited_sub_dict, create_word2vec_matrix, w2v)\n",
    "\n",
    "# Calculate metrics\n",
    "y_true = get_y_true(mapping_dataset_df)\n",
    "true_labels, predicted_labels, not_in_citation_mapping = get_true_and_predicted_task_2(y_true, y_pred)\n",
    "print(\"Mean average precision (k=10) : \", mean_average_precision(true_labels, predicted_labels, k=10))\n",
    "print(\"Mean average precision (k=100): \", mean_average_precision(true_labels, predicted_labels, k=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **V. Fine-tune TF-IDF with Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 17:53:44,581] A new study created in memory with name: no-name-44d0043d-897b-41d8-ad7b-6baba654a552\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154bb358f531446b8fbff41657635b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 17:56:54,671] Trial 0 finished with value: 0.05176587036313196 and parameters: {'sublinear_tf': True, 'norm': 'l1', 'max_features': 9387, 'n_gram_lower': 2, 'n_gram_upper': 2}. Best is trial 0 with value: 0.05176587036313196.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d042c32cff4ca1822e89771fd7ff3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 18:00:09,839] Trial 1 finished with value: 0.07348436911327604 and parameters: {'sublinear_tf': True, 'norm': 'l2', 'max_features': 1039, 'n_gram_lower': 2, 'n_gram_upper': 1}. Best is trial 1 with value: 0.07348436911327604.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48f71e1a4544600bf813888fe9ee06e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 18:04:11,291] Trial 2 finished with value: 0.05504875562242186 and parameters: {'sublinear_tf': True, 'norm': 'l2', 'max_features': 2316, 'n_gram_lower': 3, 'n_gram_upper': 1}. Best is trial 1 with value: 0.07348436911327604.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9e148ad9934b6f951e18a291eb1f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 18:05:53,521] Trial 3 finished with value: 0.12321246696699874 and parameters: {'sublinear_tf': True, 'norm': 'l2', 'max_features': 8303, 'n_gram_lower': 1, 'n_gram_upper': 1}. Best is trial 3 with value: 0.12321246696699874.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcfbf6184ff48a697801579907eef8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 18:09:59,312] Trial 4 finished with value: 0.055080564546617805 and parameters: {'sublinear_tf': True, 'norm': 'l2', 'max_features': 1819, 'n_gram_lower': 3, 'n_gram_upper': 2}. Best is trial 3 with value: 0.12321246696699874.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85a6767f10a487e820e27aef7e34961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to optimize\n",
    "    sublinear_tf = trial.suggest_categorical(\"sublinear_tf\", [True, False])\n",
    "    norm = trial.suggest_categorical(\"norm\", ['l1', 'l2'])\n",
    "    max_features = trial.suggest_int(\"max_features\", 1000, 10000)\n",
    "    n_gram_lower = trial.suggest_int(\"n_gram_lower\", 1, 3)\n",
    "    n_gram_upper = trial.suggest_int(\"n_gram_upper\", 1, 3)\n",
    "    binary = trial.suggest_categorical(\"binary\", [True, False])\n",
    "\n",
    "    if n_gram_upper < n_gram_lower:\n",
    "        n_gram_upper = n_gram_lower\n",
    "\n",
    "    # Create the vectorizer with the hyperparameters\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=sublinear_tf, \n",
    "                                 norm=norm,\n",
    "                                binary=binary,\n",
    "                                 max_features=max_features, \n",
    "                                 ngram_range=(n_gram_lower, n_gram_upper))\n",
    "\n",
    "    # Predict using the pipeline\n",
    "    y_pred = predict_pipeline(train_pairs, corpus_citing_sub_dict, corpus_cited_sub_dict, create_tfidf_matrix, vectorizer)\n",
    "    true_labels, predicted_labels, _ = get_true_and_predicted_task_2(y_true, y_pred)\n",
    "    map_10 = mean_average_precision(true_labels, predicted_labels, k=10)\n",
    "\n",
    "    return map_10\n",
    "\n",
    "# Create the study and optimize the objective function\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Print the best hyperparameters and the best MAP@10\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best Mean Average Precision (k=10):\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VI. Predictions on test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mapping_dict = load_json_data(\"predictions/task1/prediction1_k100_449.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a3c74a6dc294e46ace0cae33f904461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get citing paragraphs:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf9a06acf4842839147ffa52f0b8ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get cited paragraphs:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b3a08f21444036b1b03e342ff76eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f030401ab149d386fcae3ab74cd86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Corpus Dictionaries:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get all train pairs (id_citing, id_cited)\n",
    "test_pairs = create_pairs(test_mapping_dict, 20)\n",
    "test_citing_dataset_df = pd.DataFrame(json_citing_test)\n",
    "\n",
    "# Get paragraphs of citing and cited\n",
    "citing_ids, citing_paragraphs = get_citing_paragraphs(test_pairs, test_citing_dataset_df)\n",
    "cited_ids, cited_corpus, cited_type = get_cited_paragraphs(test_pairs, nonciting_dataset_df)\n",
    "\n",
    "# Create the citing corpus\n",
    "citing_corpus = map_ids_to_text(citing_ids, citing_paragraphs)\n",
    "\n",
    "# Clean the citing corpus\n",
    "citing_corpus = clean_corpus(citing_corpus, stem=True)\n",
    "\n",
    "# Create the citing and cited corpus dictionaries\n",
    "# Key: id_citing + \"_\" + id_cited\n",
    "corpus_citing_sub_dict = {}\n",
    "corpus_cited_sub_dict = {}\n",
    "\n",
    "for i, (citing, cited) in tqdm(enumerate(test_pairs), total=len(test_pairs), desc=\"Creating Corpus Dictionaries\"):\n",
    "    key = citing + \"_\" + cited\n",
    "\n",
    "    corpus_cited_sub = map_ids_to_text(cited_type[i], cited_corpus[i])\n",
    "    corpus_citing_sub = [citing_corpus[ind_citing(citing_corpus, citing)]]\n",
    "\n",
    "    # Clean the cited corpus\n",
    "    corpus_cited_sub = clean_corpus(corpus_cited_sub, verbose=False, stem=True)\n",
    "\n",
    "    # Add citing and cited corpus to the dictionaries\n",
    "    corpus_citing_sub_dict[key] = corpus_citing_sub\n",
    "    corpus_cited_sub_dict[key] = corpus_cited_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the citing and cited corpus dictionaries to save time\n",
    "# with open(\"datasets/corpus_citing_sub_dict_test.json\", \"w\") as file:\n",
    "#     json.dump(corpus_citing_sub_dict, file)\n",
    "\n",
    "with open(\"datasets/corpus_cited_sub_dict_test.json\", \"w\") as file:\n",
    "    json.dump(corpus_cited_sub_dict, file)\n",
    "\n",
    "# Load the citing and cited corpus dictionaries\n",
    "# with open(\"datasets/corpus_citing_sub_dict.json\", \"r\") as file:\n",
    "#     corpus_citing_sub_dict = json.load(file)\n",
    "\n",
    "# with open(\"datasets/corpus_cited_sub_dict.json\", \"r\") as file:\n",
    "#     corpus_cited_sub_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e517a9ba963d471e9ffa9a5dd3079cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predicting using CountVectorizer\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "y_pred = predict_pipeline(test_pairs, corpus_citing_sub_dict, corpus_cited_sub_dict, create_tfidf_matrix, vectorizer)\n",
    "\n",
    "# Store the result\n",
    "filename = \"prediction2_CountVec_Bin.json\"\n",
    "with open(f\"predictions/task2/{filename}\", 'w') as f:\n",
    "    json.dump(y_pred, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, stop_words='english', max_features=10000)\n",
    "y_pred = predict_pipeline(test_pairs, corpus_citing_sub_dict, corpus_cited_sub_dict, create_tfidf_matrix, vectorizer)\n",
    "\n",
    "# Store the result\n",
    "filename = \"prediction2_.json\"\n",
    "with open(f\"predictions/task2/{filename}\", 'w') as f:\n",
    "    json.dump(y_pred, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
