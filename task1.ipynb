{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <ins>**TASK 1**</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from gensim.models import Word2Vec\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Utils Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <ins>Original functions from starter_notebooks</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        contents = json.load(file)\n",
    "    return contents\n",
    "\n",
    "\n",
    "def create_tfidf_matrix(citing_dataset, nonciting_dataset, vectorizer=TfidfVectorizer()):\n",
    "    \"\"\"\n",
    "    Creates TF-IDF matrix for the given citing and non-citing datasets based on the specified text column.\n",
    "\n",
    "    Parameters:\n",
    "    citing_dataset (json)): DataFrame containing citing patents.\n",
    "    nonciting_dataset (json): DataFrame containing non-citing patents.\n",
    "    vectorizer (TfidfVectorizer, optional): TfidfVectorizer object for vectorizing text data.\n",
    "                                             Defaults to TfidfVectorizer().\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing TF-IDF matrices for citing and non-citing patents respectively.\n",
    "           (tfidf_matrix_citing, tfidf_matrix_nonciting)\n",
    "    \"\"\"\n",
    "    all_text = [patent['text'] for patent in citing_dataset + nonciting_dataset]\n",
    "\n",
    "    # Vectorizing descriptions\n",
    "    # tfidf_matrix = vectorizer.fit_transform(tqdm(all_text, desc=\"TF-IDF\"))\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_text)\n",
    "\n",
    "    # Since we're interested in similarities between citing and cited patents,\n",
    "    # we need to split the TF-IDF matrix back into two parts\n",
    "    split_index = len(citing_dataset)\n",
    "    tfidf_matrix_citing = tfidf_matrix[:split_index]\n",
    "    tfidf_matrix_nonciting = tfidf_matrix[split_index:]\n",
    "\n",
    "    # Size of vocabulary\n",
    "    # print(\"Size of vocabulary:\", len(vectorizer.vocabulary_))\n",
    "\n",
    "    return tfidf_matrix_citing, tfidf_matrix_nonciting\n",
    "\n",
    "\n",
    "\n",
    "def get_mapping_dict(mapping_df):\n",
    "    \"\"\"\n",
    "    Creates dictionary of citing ids to non-citing id based on given dataframe (which is based on providedjson)\n",
    "\n",
    "    Parameters:\n",
    "    mapping_df (DataFrame): DataFrame containing mapping between citing and cited patents\n",
    "    Returns:\n",
    "    dict: dictionary of unique citing patent ids to list of cited patent ids\n",
    "    \"\"\"\n",
    "    mapping_dict = {}\n",
    "\n",
    "    for _, row in mapping_df.iterrows():\n",
    "        key = row[0]  # Value from column 0\n",
    "        value = row[2]  # Value from column 2\n",
    "        if key in mapping_dict:\n",
    "            mapping_dict[key].append(value)\n",
    "        else:\n",
    "            mapping_dict[key] = [value]\n",
    "\n",
    "    return mapping_dict\n",
    "\n",
    "def create_corpus(corpus, text_type):\n",
    "    \"\"\"\n",
    "    Extracts text data from a corpus based on the specified text type.\n",
    "\n",
    "    Parameters:\n",
    "    corpus (list): List of dictionaries representing patent documents.\n",
    "    text_type (str): Type of text to extract ('title', 'abstract', 'claim1', 'claims', 'description', 'fulltext').\n",
    "\n",
    "    Returns:\n",
    "    list: List of dictionaries with 'id' and 'text' keys representing each document in the corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    app_ids = [doc['Application_Number'] + doc['Application_Category'] for doc in corpus]\n",
    "\n",
    "    cnt = 0 # count the number of documents without text\n",
    "    texts = []  # list of texts\n",
    "    ids_to_remove = []  # list of ids of documents without text, to remove them from the corpus\n",
    "\n",
    "    if text_type == 'title':\n",
    "        for doc in corpus:\n",
    "            try:\n",
    "                texts.append(doc['Content']['title'])\n",
    "            except: # if the document does not have a title\n",
    "                ids_to_remove.append(doc['Application_Number']+doc['Application_Category'])\n",
    "                cnt += 1\n",
    "        if cnt > 0:\n",
    "            print(f\"Number of documents without title: {cnt}\")\n",
    "\n",
    "    elif text_type == 'abstract':\n",
    "        for doc in corpus:\n",
    "            try:\n",
    "                texts.append(doc['Content']['pa01'])\n",
    "            except: # if the document does not have an abstract\n",
    "                ids_to_remove.append(doc['Application_Number']+doc['Application_Category'])\n",
    "                cnt += 1\n",
    "        if cnt > 0:\n",
    "            print(f\"Number of documents without abstract: {cnt}\")\n",
    "\n",
    "    elif text_type == 'claim1':\n",
    "        for doc in corpus:\n",
    "            try:\n",
    "                texts.append(doc['Content']['c-en-0001'])\n",
    "            except: # if the document does not have claim 1\n",
    "                ids_to_remove.append(doc['Application_Number']+doc['Application_Category'])\n",
    "                cnt += 1\n",
    "\n",
    "        if cnt > 0:\n",
    "            print(f\"Number of documents without claim 1: {cnt}\")\n",
    "\n",
    "    elif text_type == 'claims':\n",
    "        # all the values with the key starting with 'c-en-', each element in the final list is a list of claims\n",
    "        for doc in corpus:\n",
    "            doc_claims = []\n",
    "            for key in doc['Content'].keys():\n",
    "                if key.startswith('c-en-'):\n",
    "                    doc_claims.append(doc['Content'][key])\n",
    "            if len(doc_claims) == 0:    # if the document does not have any claims\n",
    "                ids_to_remove.append(doc['Application_Number']+doc['Application_Category'])\n",
    "                cnt += 1\n",
    "            else:\n",
    "                doc_text_string = ' '.join(doc_claims)\n",
    "                texts.append(doc_text_string)\n",
    "        if cnt > 0:\n",
    "            print(f\"Number of documents without claims: {cnt}\")\n",
    "\n",
    "    elif text_type == 'description':\n",
    "        # all the values with the key starting with 'p'\n",
    "        for doc in corpus:\n",
    "            doc_text = []\n",
    "            for key in doc['Content'].keys():\n",
    "                if key.startswith('p'):\n",
    "                    doc_text.append(doc['Content'][key])\n",
    "            if len(doc_text) == 0:  # if the document does not have any description\n",
    "                ids_to_remove.append(doc['Application_Number']+doc['Application_Category'])\n",
    "                cnt += 1\n",
    "            else:\n",
    "                doc_text_string = ' '.join(doc_text)\n",
    "                texts.append(doc_text_string)\n",
    "        if cnt > 0:\n",
    "            print(f\"Number of documents without description: {cnt}\")\n",
    "\n",
    "    elif text_type == 'fulltext':\n",
    "        for doc in corpus:\n",
    "            doc_text = list(doc['Content'].values())\n",
    "            doc_text_string = ' '.join(doc_text)\n",
    "            texts.append(doc_text_string)\n",
    "        if cnt > 0:\n",
    "            print(f\"Number of documents without any text: {cnt}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid text type\")\n",
    "\n",
    "    if len(ids_to_remove) > 0:\n",
    "        print(f\"Removing {len(ids_to_remove)} documents without required text\")\n",
    "        for id_ in ids_to_remove[::-1]:\n",
    "            idx = app_ids.index(id_)\n",
    "            del app_ids[idx]\n",
    "\n",
    "    # Create a list of dictionaries with app_ids and texts\n",
    "    corpus_data = [{'id': app_id, 'text': text} for app_id, text in zip(app_ids, texts)]\n",
    "\n",
    "    return corpus_data\n",
    "\n",
    "\n",
    "def get_true_and_predicted(citing_to_cited_dict, recommendations_dict):\n",
    "    \"\"\"\n",
    "    Get the true and predicted labels for the metrics calculation.\n",
    "\n",
    "    Parameters:\n",
    "    citing_to_cited_dict : dict of str : list of str\n",
    "        Mapping between citing patents and the list of their cited patents\n",
    "    recommendations_dict : dict of str : list of str\n",
    "        Mapping between citing patents and the sorted list of recommended patents\n",
    "\n",
    "    Returns:\n",
    "    list of list\n",
    "        True relevant items for each recommendation list.\n",
    "    list of list\n",
    "        Predicted recommended items for each recommendation list.\n",
    "    int\n",
    "        Number of patents not in the citation mapping\n",
    "    \"\"\"\n",
    "    # Initialize lists to store true labels and predicted labels\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    not_in_citation_mapping = 0\n",
    "\n",
    "    # Iterate over the items in both dictionaries\n",
    "    for citing_id in recommendations_dict.keys():\n",
    "        # Check if the citing_id is present in both dictionaries\n",
    "        if citing_id in citing_to_cited_dict:\n",
    "            # If yes, append the recommended items from both dictionaries to the respective lists\n",
    "            true_labels.append(citing_to_cited_dict[citing_id])\n",
    "            predicted_labels.append(recommendations_dict[citing_id])\n",
    "        else:\n",
    "            not_in_citation_mapping += 1\n",
    "\n",
    "    return true_labels, predicted_labels, not_in_citation_mapping\n",
    "\n",
    "\n",
    "\n",
    "def mean_recall_at_k(true_labels, predicted_labels, k=10):\n",
    "    \"\"\"\n",
    "    Calculate the mean Recall@k for a list of recommendations.\n",
    "\n",
    "    Parameters:\n",
    "    true_labels : list of list\n",
    "        True relevant items for each recommendation list.\n",
    "    predicted_labels : list of list\n",
    "        Predicted recommended items for each recommendation list.\n",
    "    k : int\n",
    "        Number of recommendations to consider.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        Mean Recall@k value.\n",
    "    \"\"\"\n",
    "    recalls_at_k = []\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        # Calculate Recall@k for each recommendation list\n",
    "        true_set = set(true)\n",
    "        k = min(k, len(pred))\n",
    "        relevant_count = sum(1 for item in pred[:k] if item in true_set)\n",
    "        recalls_at_k.append(relevant_count / len(true_set))\n",
    "\n",
    "    # Calculate the mean Recall@k\n",
    "    mean_recall = sum(recalls_at_k) / len(recalls_at_k)\n",
    "\n",
    "    return mean_recall\n",
    "\n",
    "def mean_inv_ranking(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculate the mean of lists of the mean inverse rank of true relevant items\n",
    "    in the lists of sorted recommended items.\n",
    "\n",
    "    Parameters:\n",
    "    true_labels : list of list\n",
    "        True relevant items for each recommendation list.\n",
    "    predicted_labels : list of list\n",
    "        Predicted recommended items for each recommendation list.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        Mean of lists of the mean inverse rank of true relevant items.\n",
    "    \"\"\"\n",
    "    mean_ranks = []\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        # Calculate the inverse rank of true relevant items\n",
    "        # in the recommendation list\n",
    "        ranks = []\n",
    "        for item in true:\n",
    "            try:\n",
    "                rank = 1 / (pred.index(item) + 1)\n",
    "            except ValueError:\n",
    "                rank = 0  # If item not found, assign 0\n",
    "            ranks.append(rank)\n",
    "\n",
    "        # Calculate the mean inverse rank of true relevant items\n",
    "        # in the recommendation list\n",
    "        mean_rank = sum(ranks) / len(ranks)\n",
    "        mean_ranks.append(mean_rank)\n",
    "\n",
    "    # Calculate the mean of the mean inverse ranks across all recommendation lists\n",
    "    mean_of_mean_ranks = sum(mean_ranks) / len(mean_ranks)\n",
    "\n",
    "    return mean_of_mean_ranks\n",
    "\n",
    "\n",
    "def mean_ranking(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculate the mean of lists of the mean rank of true relevant items\n",
    "    in the lists of sorted recommended items.\n",
    "\n",
    "    Parameters:\n",
    "    true_labels : list of list\n",
    "        True relevant items for each recommendation list.\n",
    "    predicted_labels : list of list\n",
    "        Predicted recommended items for each recommendation list.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        Mean of lists of the mean rank of true relevant items.\n",
    "    \"\"\"\n",
    "    mean_ranks = []\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        # Calculate the rank of true relevant items\n",
    "        # in the recommendation list\n",
    "        ranks = []\n",
    "        for item in true:\n",
    "            try:\n",
    "                rank = pred.index(item) + 1\n",
    "            except ValueError:\n",
    "                rank = len(pred)  # If item not found, assign the length of the list\n",
    "            ranks.append(rank)\n",
    "\n",
    "        # Calculate the mean rank of true relevant items\n",
    "        # in the recommendation list\n",
    "        mean_rank = sum(ranks) / len(ranks)\n",
    "        mean_ranks.append(mean_rank)\n",
    "\n",
    "    # Calculate the mean of the mean ranks across all recommendation lists\n",
    "    mean_of_mean_ranks = sum(mean_ranks) / len(mean_ranks)\n",
    "\n",
    "    return mean_of_mean_ranks\n",
    "\n",
    "\n",
    "\n",
    "def mean_average_precision(true_labels, predicted_labels, k=10):\n",
    "    \"\"\"\n",
    "    Calculate the mean Average Precision for a list of recommendations.\n",
    "\n",
    "    Parameters:\n",
    "    true_labels : list of list\n",
    "        True relevant items for each recommendation list.\n",
    "    predicted_labels : list of list\n",
    "        Predicted recommended items for each recommendation list.\n",
    "    k : int\n",
    "        Number of recommendations to consider.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        Mean Average Precision value.\n",
    "    \"\"\"\n",
    "    average_precisions = []\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        # Calculate Average Precision for each recommendation list\n",
    "        true_set = set(true)\n",
    "        precision_at_k = []\n",
    "        relevant_count = 0\n",
    "        for i, item in enumerate(pred[:k]):\n",
    "            if item in true_set:\n",
    "                relevant_count += 1\n",
    "                precision_at_k.append(relevant_count / (i + 1))\n",
    "        average_precision = sum(precision_at_k) / len(true_set)\n",
    "        average_precisions.append(average_precision)\n",
    "\n",
    "    # Calculate the mean Average Precision\n",
    "    mean_average_precision = sum(average_precisions) / len(average_precisions)\n",
    "\n",
    "    return mean_average_precision\n",
    "\n",
    "def top_k_ranks(citing, cited, cosine_similarities, k=10):\n",
    "    # Create a dictionary to store the top k ranks for each citing patent\n",
    "    top_k_ranks = {}\n",
    "    for i, content_id in enumerate(citing):\n",
    "        top_k_ranks[content_id['id']] = [cited[j]['id'] for j in np.argsort(cosine_similarities[i])[::-1][:k]]\n",
    "    return top_k_ranks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <ins>BM25</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <ins>Customs functions</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords=True, remove_digits=False, stem=False, lemmatize=False):\n",
    "    \"\"\"\n",
    "    Clean the text data by converting to lowercase, removing stopwords, stemming, and removing special characters.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Text data to clean.\n",
    "    remove_stopwords (bool, optional): Whether to remove stopwords. Defaults to True.\n",
    "    stem (bool, optional): Whether to perform stemming. Defaults to False.\n",
    "    lemmatize (bool, optional): Whether to perform lemmatization. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    str: Cleaned text data.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        text = ' '.join([word for word in text.split()\n",
    "                        if word not in stop_words])\n",
    "\n",
    "    # Remove (1), (2), etc patterns\n",
    "    text = re.sub(r'\\(([0-9])+\\)', '', text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove special characters\n",
    "    if not remove_digits:\n",
    "        text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "\n",
    "    # Keep only alphabetic characters\n",
    "    else:\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    if stem:\n",
    "        text = stem_text(text)\n",
    "    \n",
    "    elif lemmatize:\n",
    "        text = lemmatize_text(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_corpus(corpus, verbose=True, remove_digits=False, stem=False, lemmatize=False):\n",
    "    \"\"\"\n",
    "    Clean the text data in the corpus by converting to lowercase, removing stopwords, stemming, and removing special characters.\n",
    "\n",
    "    Parameters:\n",
    "    corpus (list): List of dictionaries representing patent documents.\n",
    "    verbose (bool, optional): Whether to show progress bar. Defaults to True.\n",
    "    stem (bool, optional): Whether to perform stemming. Defaults to False.\n",
    "    lemmatize (bool, optional): Whether to perform lemmatization. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    list: List of dictionaries with 'id' and 'text' keys representing each document in the corpus.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        for patent in tqdm(corpus, desc=\"Cleaning\"):\n",
    "            patent['text'] = clean_text(patent['text'], stem=stem, lemmatize=lemmatize, remove_digits=remove_digits)\n",
    "    \n",
    "    else:\n",
    "        for patent in corpus:\n",
    "            patent['text'] = clean_text(patent['text'], stem=stem, lemmatize=lemmatize, remove_digits=remove_digits)\n",
    "\n",
    "\n",
    "def stem_text(text):\n",
    "    \"\"\"\n",
    "    Perform stemming on the given text data.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Text data to perform stemming on.\n",
    "\n",
    "    Returns:\n",
    "    str: Stemmed text data.\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    return ' '.join([ps.stem(word) for word in text.split()])\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    Perform lemmatization on the given text data.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Text data to perform lemmatization on.\n",
    "\n",
    "    Returns:\n",
    "    str: Lemmatized text data.\n",
    "    \"\"\"\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "def create_word2vec_matrix(citing_dataset, nonciting_dataset, word2vec_model):\n",
    "    \"\"\"\n",
    "    Creates Word2Vec matrix for the given citing and non-citing datasets based on the specified Word2Vec model.\n",
    "\n",
    "    Parameters:\n",
    "    citing_dataset (list of str): List of citing patents' text.\n",
    "    nonciting_dataset (list of str): List of non-citing patents' text.\n",
    "    word2vec_model (Word2Vec): Pre-trained Word2Vec model.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing Word2Vec matrices for citing and non-citing patents respectively.\n",
    "           (word2vec_matrix_citing, word2vec_matrix_nonciting)\n",
    "    \"\"\"\n",
    "    def get_average_vector(text, model):\n",
    "        words = text.split()\n",
    "        vectors = [model.wv.get_vector(word) for word in words if word in model.wv]\n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            # If no words in the document are found in the model vocabulary,\n",
    "            # return a zero vector of the same dimensionality\n",
    "            return np.zeros(model.vector_size)\n",
    "\n",
    "    # Convert text data to Word2Vec vectors\n",
    "    word2vec_matrix_citing = np.array([get_average_vector(patent[\"text\"], word2vec_model) for patent in citing_dataset])\n",
    "    word2vec_matrix_nonciting = np.array([get_average_vector(patent[\"text\"], word2vec_model) for patent in nonciting_dataset])\n",
    "\n",
    "    return word2vec_matrix_citing, word2vec_matrix_nonciting\n",
    "\n",
    "def create_bert_matrix(citing_dataset, nonciting_dataset, model_name='bert-base-uncased'):\n",
    "    \"\"\"\n",
    "    Creates BERT matrix for the given citing and non-citing datasets based on the pre-trained BERT model.\n",
    "\n",
    "    Parameters:\n",
    "    citing_dataset (list of str): List of citing patents' text.\n",
    "    nonciting_dataset (list of str): List of non-citing patents' text.\n",
    "    model_name (str, optional): Name of the pre-trained BERT model. Defaults to 'bert-base-uncased'.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing BERT matrices for citing and non-citing patents respectively.\n",
    "           (bert_matrix_citing, bert_matrix_nonciting)\n",
    "    \"\"\"\n",
    "    # Load pre-trained BERT tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Function to tokenize and encode text\n",
    "    def tokenize_and_encode(text):\n",
    "        input_ids = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=512, padding='max_length', return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "        # Take the output of the [CLS] token as the sentence representation\n",
    "        return outputs[0][:, 0, :].numpy()\n",
    "\n",
    "    # Encode the text data using the BERT model\n",
    "    bert_matrix_citing = [tokenize_and_encode(text) for text in citing_dataset]\n",
    "    bert_matrix_nonciting = [tokenize_and_encode(text) for text in nonciting_dataset]\n",
    "\n",
    "    return bert_matrix_citing, bert_matrix_nonciting\n",
    "\n",
    "def predict_pipeline_task1(corpus_citing, corpus_nonciting, vectorizer, create_matrix=create_tfidf_matrix, cleaning_data=False, k=100):\n",
    "    \"\"\"\n",
    "    Pipeline for predicting the top k ranks for each citing patent based on the cosine similarities between citing and non-citing patents.\n",
    "\n",
    "    Parameters:\n",
    "    corpus_citing (list): List of dictionaries representing citing patents.\n",
    "    corpus_nonciting (list): List of dictionaries representing non-citing patents.\n",
    "    vectorizer: Object for vectorizing text data, e.g., TfidfVectorizer.\n",
    "    create_matrix (function, optional): Function for creating the matrix. Defaults to create_tfidf_matrix.\n",
    "    cleaning_data (bool, optional): Whether to clean the text data. Defaults to False.\n",
    "    k (int, optional): Number of recommendations to consider. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing the top k ranks for each citing patent.\n",
    "    \"\"\"\n",
    "    if cleaning_data:\n",
    "        print(\"Cleaning data...\")\n",
    "        clean_corpus(corpus_citing)\n",
    "        clean_corpus(corpus_nonciting)\n",
    "\n",
    "    vec_citing, vec_nonciting = create_matrix(corpus_citing, corpus_nonciting, vectorizer)\n",
    "    cosine_similarities = linear_kernel(vec_citing, vec_nonciting)\n",
    "    top_k_ranks_dict = top_k_ranks(corpus_citing, corpus_nonciting, cosine_similarities, k=k)\n",
    "\n",
    "    return top_k_ranks_dict\n",
    "\n",
    "def show_results(predicted_ranking, mapping_dict, k=100):\n",
    "    \"\"\"\n",
    "    Show the results of the evaluation metrics.\n",
    "\n",
    "    Parameters:\n",
    "    predicted_ranking (dict): Dictionary containing the top k ranks for each citing patent.\n",
    "    mapping_dict (dict): Dictionary of citing ids to non-citing id based on given dataframe.\n",
    "    k (int, optional): Number of recommendations to consider. Defaults to 100.\n",
    "    \"\"\"\n",
    "    print(\"\\n\")\n",
    "    # Get the true and predicted labels\n",
    "    true_labels, predicted_labels, not_in_citation_mapping = get_true_and_predicted(mapping_dict, predicted_ranking)\n",
    "\n",
    "    if not_in_citation_mapping > 0:\n",
    "        print(f\"Number of patents not in the citation mapping: {not_in_citation_mapping}\")\n",
    "\n",
    "    # Calculate the evaluation metrics\n",
    "    mat10 = mean_recall_at_k(true_labels, predicted_labels, 10)\n",
    "    mat20 = mean_recall_at_k(true_labels, predicted_labels, 20)\n",
    "    mat50 = mean_recall_at_k(true_labels, predicted_labels, 50)\n",
    "    mat100 = mean_recall_at_k(true_labels, predicted_labels, 100)\n",
    "    mean_avg_precision = mean_average_precision(true_labels, predicted_labels, k)\n",
    "\n",
    "    # Display the evaluation metrics\n",
    "    print(f\"Mean Recal@10 : {mat10:.4f}\")\n",
    "    print(f\"Mean Recal@20 : {mat20:.4f}\")\n",
    "    print(f\"Mean Recal@50 : {mat50:.4f}\")\n",
    "    print(f\"Mean Recal@100: {mat100:.4f}\")\n",
    "    print(f\"Mean Average Precision: {mean_avg_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_citing_train = load_json_data(\"./datasets/Content_JSONs/Citing_2020_Cleaned_Content_12k/Citing_Train_Test/citing_TRAIN.json\")\n",
    "json_citing_test = load_json_data(\"./datasets/Content_JSONs/Citing_2020_Cleaned_Content_12k/Citing_Train_Test/citing_TEST.json\")\n",
    "\n",
    "json_nonciting = load_json_data(\"./datasets/Content_JSONs/Cited_2020_Uncited_2010-2019_Cleaned_Content_22k/CLEANED_CONTENT_DATASET_cited_patents_by_2020_uncited_2010-2019.json\")\n",
    "json_citing_to_cited = load_json_data(\"./datasets/Citation_JSONs/Citation_Train.json\")\n",
    "json_citing_id = load_json_data(\"./Citing_ID_List_Test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "citing_dataset_df = pd.DataFrame(json_citing_train)\n",
    "nonciting_dataset_df = pd.DataFrame(json_nonciting)\n",
    "mapping_dataset_df = pd.DataFrame(json_citing_to_cited)\n",
    "\n",
    "mapping_dict = get_mapping_dict(mapping_dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **I. Basic TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic TF-IDF matrix\n",
      "Full text for citing and non-citing patents not cleaned.\n",
      "\n",
      "\n",
      "Mean Recal@10 : 0.6651\n",
      "Mean Recal@20 : 0.7456\n",
      "Mean Recal@50 : 0.8320\n",
      "Mean Recal@100: 0.8932\n",
      "Mean Average Precision: 0.4571\n"
     ]
    }
   ],
   "source": [
    "# 3m\n",
    "print(\"Basic TF-IDF matrix\")\n",
    "print(\"Full text for citing and non-citing patents not cleaned.\")\n",
    "\n",
    "corpus_citing_train = create_corpus(json_citing_train, \"fulltext\")\n",
    "corpus_nonciting = create_corpus(json_nonciting, \"fulltext\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, sublinear_tf=True)\n",
    "predicted_ranking = predict_pipeline_task1(corpus_citing_train, corpus_nonciting, vectorizer)\n",
    "\n",
    "show_results(predicted_ranking, mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic TF-IDF matrix\n",
      "Claims for citing and description for non-citing patents not cleaned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mean Recal@10 : 0.6712\n",
      "Mean Recal@20 : 0.7483\n",
      "Mean Recal@50 : 0.8460\n",
      "Mean Recal@100: 0.9002\n",
      "Mean Average Precision: 0.4518\n"
     ]
    }
   ],
   "source": [
    "# 1m40s\n",
    "print(\"Basic TF-IDF matrix\")\n",
    "print(\"Claims for citing and description for non-citing patents not cleaned.\")\n",
    "\n",
    "corpus_citing_train = create_corpus(json_citing_train, \"claims\")\n",
    "corpus_nonciting = create_corpus(json_nonciting, \"description\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, sublinear_tf=True)\n",
    "predicted_ranking = predict_pipeline_task1(corpus_citing_train, corpus_nonciting, vectorizer)\n",
    "\n",
    "show_results(predicted_ranking, mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic TF-IDF matrix\n",
      "Full text for citing and description for non-citing patents stemmed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7becde9c5a4310ab821bc71313d504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning:   0%|          | 0/6831 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd11274b4bbf4e678f51008a5ea3e0c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning:   0%|          | 0/16837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mean Recal@10 : 0.6651\n",
      "Mean Recal@20 : 0.7479\n",
      "Mean Recal@50 : 0.8345\n",
      "Mean Recal@100: 0.8883\n",
      "Mean Average Precision: 0.4516\n"
     ]
    }
   ],
   "source": [
    "# 30m\n",
    "# Keep digits\n",
    "print(\"Basic TF-IDF matrix\")\n",
    "print(\"Full text for citing and description for non-citing patents stemmed.\")\n",
    "\n",
    "corpus_citing_train = create_corpus(json_citing_train, \"fulltext\")\n",
    "corpus_nonciting = create_corpus(json_nonciting, \"fulltext\")\n",
    "\n",
    "clean_corpus(corpus_citing_train, verbose=True, stem=True)\n",
    "clean_corpus(corpus_nonciting, verbose=True, stem=True)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, sublinear_tf=True)\n",
    "predicted_ranking = predict_pipeline_task1(corpus_citing_train, corpus_nonciting, vectorizer)\n",
    "\n",
    "show_results(predicted_ranking, mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic TF-IDF matrix\n",
      "Full text for citing and description for non-citing patents stemmed and digits removed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9cc2f6a43b41d48e93604be64e9925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning:   0%|          | 0/6831 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a324f6c8d9ce4db98006233fbfc8d2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning:   0%|          | 0/16837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mean Recal@10 : 0.6633\n",
      "Mean Recal@20 : 0.7457\n",
      "Mean Recal@50 : 0.8326\n",
      "Mean Recal@100: 0.8858\n",
      "Mean Average Precision: 0.4493\n"
     ]
    }
   ],
   "source": [
    "# 30m\n",
    "# Remove digits\n",
    "print(\"Basic TF-IDF matrix\")\n",
    "print(\"Full text for citing and description for non-citing patents stemmed and digits removed.\")\n",
    "\n",
    "corpus_citing_train = create_corpus(json_citing_train, \"fulltext\")\n",
    "corpus_nonciting = create_corpus(json_nonciting, \"fulltext\")\n",
    "\n",
    "clean_corpus(corpus_citing_train, verbose=True, stem=True, remove_digits=True)\n",
    "clean_corpus(corpus_nonciting, verbose=True, stem=True, remove_digits=True)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, sublinear_tf=True)\n",
    "predicted_ranking = predict_pipeline_task1(corpus_citing_train, corpus_nonciting, vectorizer)\n",
    "\n",
    "show_results(predicted_ranking, mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic TF-IDF matrix\n",
      "Full text for citing and non-citing patents not cleaned.\n",
      "\n",
      "\n",
      "Mean Recal@10 : 0.6592\n",
      "Mean Recal@20 : 0.7442\n",
      "Mean Recal@50 : 0.8300\n",
      "Mean Recal@100: 0.8901\n",
      "Mean Average Precision: 0.4528\n"
     ]
    }
   ],
   "source": [
    "# 4m\n",
    "print(\"Basic TF-IDF matrix\")\n",
    "print(\"Full text for citing and non-citing patents cleaned.\")\n",
    "\n",
    "corpus_citing_train = create_corpus(json_citing_train, \"fulltext\")\n",
    "corpus_nonciting = create_corpus(json_nonciting, \"fulltext\")\n",
    "\n",
    "clean_corpus(corpus_citing_train, verbose=False)\n",
    "clean_corpus(corpus_nonciting, verbose=False)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, sublinear_tf=True)\n",
    "predicted_ranking = predict_pipeline_task1(corpus_citing_train, corpus_nonciting, vectorizer)\n",
    "\n",
    "show_results(predicted_ranking, mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic TF-IDF matrix\n",
      "Full text for citing and non-citing patents not cleaned.\n",
      "\n",
      "\n",
      "Mean Recal@10 : 0.6864\n",
      "Mean Recal@20 : 0.7703\n",
      "Mean Recal@50 : 0.8541\n",
      "Mean Recal@100: 0.9067\n",
      "Mean Average Precision: 0.4778\n"
     ]
    }
   ],
   "source": [
    "# 3m\n",
    "# Remove digits in the text\n",
    "print(\"Basic TF-IDF matrix\")\n",
    "print(\"Full text for citing and non-citing patents cleaned and digits removed.\")\n",
    "\n",
    "corpus_citing_train = create_corpus(json_citing_train, \"fulltext\")\n",
    "corpus_nonciting = create_corpus(json_nonciting, \"fulltext\")\n",
    "\n",
    "clean_corpus(corpus_citing_train, verbose=False, remove_digits=True)\n",
    "clean_corpus(corpus_nonciting, verbose=False, remove_digits=True)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, sublinear_tf=True)\n",
    "predicted_ranking = predict_pipeline_task1(corpus_citing_train, corpus_nonciting, vectorizer)\n",
    "\n",
    "show_results(predicted_ranking, mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic TF-IDF matrix\n",
      "Claims for citing and description for non-citing patents cleaned and digits removed.\n",
      "\n",
      "\n",
      "Mean Recal@10 : 0.6856\n",
      "Mean Recal@20 : 0.7644\n",
      "Mean Recal@50 : 0.8579\n",
      "Mean Recal@100: 0.9104\n",
      "Mean Average Precision: 0.4636\n"
     ]
    }
   ],
   "source": [
    "# 2m20\n",
    "# Remove digits in the text\n",
    "print(\"Basic TF-IDF matrix\")\n",
    "print(\"Claims for citing and description for non-citing patents cleaned and digits removed.\")\n",
    "\n",
    "corpus_citing_train = create_corpus(json_citing_train, \"claims\")\n",
    "corpus_nonciting = create_corpus(json_nonciting, \"description\")\n",
    "\n",
    "clean_corpus(corpus_citing_train, verbose=False, remove_digits=True)\n",
    "clean_corpus(corpus_nonciting, verbose=False, remove_digits=True)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, sublinear_tf=True)\n",
    "predicted_ranking = predict_pipeline_task1(corpus_citing_train, corpus_nonciting, vectorizer)\n",
    "\n",
    "show_results(predicted_ranking, mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic TF-IDF matrix\n",
      "Claims for citing and description for non-citing patents cleaned and digits removed.\n",
      "\n",
      "\n",
      "Mean Recal@10 : 0.6938\n",
      "Mean Recal@20 : 0.7754\n",
      "Mean Recal@50 : 0.8654\n",
      "Mean Recal@100: 0.9148\n",
      "Mean Average Precision: 0.4688\n"
     ]
    }
   ],
   "source": [
    "# 3m\n",
    "# Remove digits in the text\n",
    "print(\"Basic TF-IDF matrix\")\n",
    "print(\"Claims for citing and description for non-citing patents cleaned and digits removed.\")\n",
    "\n",
    "corpus_citing_train = create_corpus(json_citing_train, \"claims\")\n",
    "corpus_nonciting = create_corpus(json_nonciting, \"description\")\n",
    "\n",
    "clean_corpus(corpus_citing_train, verbose=False, remove_digits=True, stem=True)\n",
    "clean_corpus(corpus_nonciting, verbose=False, remove_digits=True, stem=True)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, sublinear_tf=True)\n",
    "predicted_ranking = predict_pipeline_task1(corpus_citing_train, corpus_nonciting, vectorizer)\n",
    "\n",
    "show_results(predicted_ranking, mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the citing and cited corpus dictionaries to save time\n",
    "# with open(\"datasets/corpus/task1/citing_train_fulltext_stemmed.json\", \"w\") as file:\n",
    "#     json.dump(corpus_citing_train, file)\n",
    "\n",
    "# with open(\"datasets/corpus/task1/nonciting_fulltext_stemmed.json\", \"w\") as file:\n",
    "#     json.dump(corpus_nonciting, file)\n",
    "\n",
    "with open(\"datasets/corpus/task1/citing_train_fulltext_stemmed.json\", \"r\") as file:\n",
    "    corpus_citing_train = json.load(file)\n",
    "\n",
    "with open(\"datasets/corpus/task1/nonciting_fulltext_stemmed.json\", \"r\") as file:\n",
    "    corpus_nonciting = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede5d6e0009c494dafd2d1162182ca03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning:   0%|          | 0/6831 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d02773f3e94c1ba0a6eb5d3eea73d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning:   0%|          | 0/16837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_corpus(corpus_citing_train, remove_digits=True)\n",
    "clean_corpus(corpus_nonciting, remove_digits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mean Recal@10 : 0.7057\n",
      "Mean Recal@20 : 0.7824\n",
      "Mean Recal@50 : 0.8645\n",
      "Mean Recal@100: 0.9120\n",
      "Mean Average Precision: 0.4844\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, sublinear_tf=True)\n",
    "predicted_ranking = predict_pipeline_task1(corpus_citing_train, corpus_nonciting, vectorizer)\n",
    "\n",
    "show_results(predicted_ranking, mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0ba5b5e9fe4eff8138e85866cdfcbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning:   0%|          | 0/6831 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e6e1fd7ad414a2d951efb3cd19e854c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning:   0%|          | 0/16837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_citing_train = create_corpus(json_citing_train, \"fulltext\")\n",
    "corpus_nonciting = create_corpus(json_nonciting, \"fulltext\")\n",
    "\n",
    "clean_corpus(corpus_citing_train, remove_digits=True, stem=True)\n",
    "clean_corpus(corpus_nonciting, remove_digits=True, stem=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **II. Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the citing and cited corpus dictionaries containing stemmed of fulltext\n",
    "# with open(\"datasets/predicted_ranking/task1/fulltext_stemmed.json\", \"r\") as file:\n",
    "#     corpus_citing_train = json.load(file)\n",
    "\n",
    "# with open(\"datasets/predicted_ranking/task1/fulltext_stemmed.json\", \"r\") as file:\n",
    "#     corpus_nonciting = json.load(file)\n",
    "\n",
    "corpus_citing_train = create_corpus(json_citing_train, \"fulltext\")\n",
    "corpus_nonciting = create_corpus(json_nonciting, \"fulltext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_sentences = []\n",
    "\n",
    "for corpus in corpus_citing_train + corpus_nonciting:\n",
    "    w2v_sentences.append(corpus['text'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mean Recal@10 : 0.0203\n",
      "Mean Recal@20 : 0.0388\n",
      "Mean Recal@50 : 0.0715\n",
      "Mean Recal@100: 0.1201\n",
      "Mean Average Precision: 0.0100\n"
     ]
    }
   ],
   "source": [
    "# 20m-\n",
    "print(\"Basic Word2Vec\")\n",
    "print(\"Full text for citing and description for non-citing patents.\")\n",
    "\n",
    "vectorizer = Word2Vec(sentences=w2v_sentences, workers=4)\n",
    "predicted_ranking = predict_pipeline_task1(corpus_citing_train, corpus_nonciting, vectorizer, create_matrix=create_word2vec_matrix)\n",
    "\n",
    "show_results(predicted_ranking, mapping_dict)\n",
    "\n",
    "\n",
    "show_results(predicted_ranking, mapping_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **III. Word2Vec Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    trial.suggest_int(\"vector_size\", 100, 300)\n",
    "    # trial.suggest_int(\"window\", 1, 10)\n",
    "    # trial.suggest_int(\"min_count\", 1, 10)\n",
    "    # trial.suggest_float(\"alpha\", 0.025, 0.1)\n",
    "    # trial.suggest_float(\"min_alpha\", 0.0001, 0.01)\n",
    "    # trial.suggest_float(\"sample\", 0, 1e-5)\n",
    "\n",
    "    vectorizer = Word2Vec(**trial.params, sentences=w2v_sentences, workers=4)\n",
    "    predicted_ranking = predict_pipeline_task1(corpus_citing_train, corpus_nonciting, vectorizer, create_matrix=create_word2vec_matrix)\n",
    "\n",
    "    y_true, y_pred, _ = get_true_and_predicted(mapping_dict, predicted_ranking)\n",
    "\n",
    "    return mean_recall_at_k(y_true, y_pred, 100)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(study.best_params)\n",
    "print(study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IV. BM25**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25\n",
      "Claims for citing and description for non-citing patents.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'create_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBM25\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClaims for citing and description for non-citing patents.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m corpus_citing_train \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_corpus\u001b[49m(json_citing_train, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaims\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m corpus_nonciting \u001b[38;5;241m=\u001b[39m create_corpus(json_nonciting, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"BM25\")\n",
    "print(\"Claims for citing and description for non-citing patents.\")\n",
    "\n",
    "corpus_citing_train = create_corpus(json_citing_train, \"claims\")\n",
    "corpus_nonciting = create_corpus(json_nonciting, \"description\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, sublinear_tf=True)\n",
    "predicted_ranking = predict_pipeline_task1(corpus_citing_train, corpus_nonciting, vectorizer, create_matrix=create_bm25_matrix)\n",
    "\n",
    "show_results(predicted_ranking, mapping_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IV. BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -m-\n",
    "print(\"BERT\")\n",
    "print(\"Full text for citing and description for non-citing patents.\")\n",
    "\n",
    "corpus_citing_train = create_corpus(json_citing_train, \"fulltext\")\n",
    "corpus_nonciting = create_corpus(json_nonciting, \"fulltext\")\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "predicted_ranking = predict_pipeline_task1(corpus_citing_train, corpus_nonciting, model_name, create_matrix=create_bert_matrix)\n",
    "\n",
    "show_results(predicted_ranking, mapping_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Make predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic TF-IDF matrix\n",
      "Full text for citing and non-citing patents cleaned and remove digits.\n"
     ]
    }
   ],
   "source": [
    "# 2m2 - score mAP 0.471\n",
    "# Remove digits in the text\n",
    "print(\"Basic TF-IDF matrix\")\n",
    "print(\"Full text for citing and non-citing patents cleaned and remove digits.\")\n",
    "\n",
    "corpus_citing_test = create_corpus(json_citing_test, \"fulltext\")\n",
    "corpus_nonciting = create_corpus(json_nonciting, \"fulltext\")\n",
    "\n",
    "clean_corpus(corpus_citing_test, verbose=False, remove_digits=True)\n",
    "clean_corpus(corpus_nonciting, verbose=False, remove_digits=True)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, sublinear_tf=True)\n",
    "predicted_ranking = predict_pipeline_task1(corpus_citing_test, corpus_nonciting, vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predictions/task1/prediction1_fulltext_cleaned_nodigitnew.json', 'w') as f:\n",
    "    json.dump(predicted_ranking, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
